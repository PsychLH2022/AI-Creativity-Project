{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from keras_tuner import HyperModel, Hyperband\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def cal_acc(df_test, labels):\n",
    "    # add labels to test data\n",
    "    df_test['pred'] = labels\n",
    "\n",
    "    # calculate accuracy\n",
    "    correct = 0\n",
    "    for index, row in df_test.iterrows():\n",
    "        if row['pred'] == row['label']:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(df_test)\n",
    "    print('Accuracy average: {}'.format(accuracy))\n",
    "\n",
    "    # calculate accuracy per class\n",
    "    correct_funny = 0\n",
    "    correct_somewhat_funny = 0\n",
    "    correct_not_funny = 0\n",
    "    total_funny = 0\n",
    "    total_somewhat_funny = 0\n",
    "    total_not_funny = 0\n",
    "\n",
    "    for index, row in df_test.iterrows():\n",
    "        if row['label'] == 2:\n",
    "            total_funny += 1\n",
    "            if row['pred'] == 2:\n",
    "                correct_funny += 1\n",
    "        elif row['label'] == 1:\n",
    "            total_somewhat_funny += 1\n",
    "            if row['pred'] == 1:\n",
    "                correct_somewhat_funny += 1\n",
    "        elif row['label'] == 0:\n",
    "            total_not_funny += 1\n",
    "            if row['pred'] == 0:\n",
    "                correct_not_funny += 1\n",
    "\n",
    "    accuracy_funny = correct_funny / total_funny\n",
    "    accuracy_somewhat_funny = correct_somewhat_funny / total_somewhat_funny\n",
    "    accuracy_not_funny = correct_not_funny / total_not_funny\n",
    "\n",
    "    print('Accuracy funny: {}'.format(accuracy_funny))\n",
    "    print('Accuracy somewhat funny: {}'.format(accuracy_somewhat_funny))\n",
    "    print('Accuracy not funny: {}'.format(accuracy_not_funny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2985, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "df_train = pd.read_csv('data/train_set_850-870_top30_features.csv')\n",
    "data_train = df_train.drop(['mean', 'label'], axis=1)\n",
    "print(data_train.shape)\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_csv('data/test_set_850-870_top30_features.csv')\n",
    "data_test = df_test.drop(['mean', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2\n",
      "1       2\n",
      "2       2\n",
      "3       2\n",
      "4       2\n",
      "       ..\n",
      "2980    0\n",
      "2981    0\n",
      "2982    0\n",
      "2983    0\n",
      "2984    0\n",
      "Name: label, Length: 2985, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check 'funny' exits in label\n",
    "if 'funny' in df_train['label'].unique():\n",
    "    # recode labels\n",
    "    df_train['label'] = df_train['label'].replace('funny', 2)\n",
    "    df_train['label'] = df_train['label'].replace('somewhat_funny', 1)\n",
    "    df_train['label'] = df_train['label'].replace('not_funny', 0)\n",
    "    df_test['label'] = df_test['label'].replace('funny', 2)\n",
    "    df_test['label'] = df_test['label'].replace('somewhat_funny', 1)\n",
    "    df_test['label'] = df_test['label'].replace('not_funny', 0)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# Converting Labels to Categorical in train set\n",
    "y_train_categorical = df_train['label']\n",
    "print(y_train_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=hp.Int('unit', min_value=64, max_value=256, step=32),\n",
    "                        activation=hp.Choice('activation', values=['relu', 'tanh']),\n",
    "                        input_shape=self.input_shape,\n",
    "                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=hp.Choice('l1', values=[0.01, 0.001, 0.0001, 0.0]),\n",
    "                                                                        l2=hp.Choice('l2', values=[0.01, 0.001, 0.0001, 0.0]))))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 03s]\n",
      "val_accuracy: 0.35343384742736816\n",
      "\n",
      "Best val_accuracy So Far: 0.4036850929260254\n",
      "Total elapsed time: 00h 00m 40s\n"
     ]
    }
   ],
   "source": [
    "# Prepare your data\n",
    "X = data_train\n",
    "y = y_train_categorical\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "best_hyperparams_per_fold = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(X)):\n",
    "    print(f\"Running tuning for fold {fold + 1}\")\n",
    "\n",
    "    # Split data into train and validation for the current fold\n",
    "    X_train, X_val = X.loc[train_indices,], X.loc[val_indices,]\n",
    "    y_train, y_val = y.loc[train_indices,], y.loc[val_indices,]\n",
    "\n",
    "    # Define the hypermodel\n",
    "    hypermodel = MyHyperModel(input_shape=X_train.shape[1:], num_classes=3)\n",
    "\n",
    "    # Initialize the Hyperband tuner\n",
    "    tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=20,\n",
    "        directory=f'result/my_dir_{fold}',\n",
    "        project_name='hyperparameter_tuning',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    # Start the tuning process\n",
    "    tuner.search(X_train, y_train, validation_data=(X_val, y_val), class_weight={0: 1, 1: 1, 2: 1})\n",
    "\n",
    "    # Store the top 3 best hyperparameters of this fold\n",
    "    top_3_hyperparams_per_fold = tuner.get_best_hyperparameters(num_trials=3)\n",
    "    best_hyperparams_per_fold.append(top_3_hyperparams_per_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'unit': 96, 'activation': 'relu', 'l1': 0.001, 'l2': 0.0, 'dropout': 0.30000000000000004, 'learning_rate': 0.001, 'tuner/epochs': 7, 'tuner/initial_epoch': 0, 'tuner/bracket': 1, 'tuner/round': 0}\n",
      "{'unit': 160, 'activation': 'relu', 'l1': 0.0001, 'l2': 0.0, 'dropout': 0.30000000000000004, 'learning_rate': 0.001, 'tuner/epochs': 20, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
      "{'unit': 192, 'activation': 'tanh', 'l1': 0.001, 'l2': 0.0001, 'dropout': 0.4, 'learning_rate': 0.001, 'tuner/epochs': 20, 'tuner/initial_epoch': 7, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0012'}\n",
      "{'unit': 192, 'activation': 'relu', 'l1': 0.0001, 'l2': 0.0, 'dropout': 0.4, 'learning_rate': 0.001, 'tuner/epochs': 3, 'tuner/initial_epoch': 0, 'tuner/bracket': 2, 'tuner/round': 0}\n",
      "{'unit': 96, 'activation': 'relu', 'l1': 0.0, 'l2': 0.01, 'dropout': 0.2, 'learning_rate': 0.001, 'tuner/epochs': 20, 'tuner/initial_epoch': 7, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0013'}\n"
     ]
    }
   ],
   "source": [
    "# select best hyperparameters from best_hyperparams_per_fold\n",
    "best_hyperparams = []\n",
    "for top_hyperparams in best_hyperparams_per_fold:\n",
    "    best_hyperparams.append(top_hyperparams[0])\n",
    "\n",
    "# show best hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "for top_hyperparam in best_hyperparams:\n",
    "    print(top_hyperparam.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "75/75 [==============================] - 1s 2ms/step - loss: 1.7406 - accuracy: 0.3476 - val_loss: 2.5332 - val_accuracy: 0.0586\n",
      "Epoch 2/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5233 - accuracy: 0.3924 - val_loss: 2.4737 - val_accuracy: 0.0168\n",
      "Epoch 3/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4287 - accuracy: 0.4129 - val_loss: 1.2861 - val_accuracy: 0.3518\n",
      "Epoch 4/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3846 - accuracy: 0.4058 - val_loss: 1.9238 - val_accuracy: 0.0385\n",
      "Epoch 5/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3369 - accuracy: 0.4146 - val_loss: 2.0306 - val_accuracy: 0.0184\n",
      "Epoch 6/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3031 - accuracy: 0.4322 - val_loss: 1.8484 - val_accuracy: 0.0318\n",
      "Epoch 7/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3058 - accuracy: 0.4146 - val_loss: 2.2577 - val_accuracy: 0.0101\n",
      "Epoch 8/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.2678 - accuracy: 0.4338 - val_loss: 2.0272 - val_accuracy: 0.0168\n",
      "Epoch 9/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.2574 - accuracy: 0.4221 - val_loss: 2.3135 - val_accuracy: 0.0084\n",
      "Epoch 10/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.2518 - accuracy: 0.4313 - val_loss: 2.0017 - val_accuracy: 0.0168\n",
      "Epoch 11/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.2353 - accuracy: 0.4393 - val_loss: 2.0018 - val_accuracy: 0.0235\n",
      "Epoch 12/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.2249 - accuracy: 0.4506 - val_loss: 1.9998 - val_accuracy: 0.0168\n",
      "Epoch 13/50\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.2160 - accuracy: 0.4351 - val_loss: 2.0949 - val_accuracy: 0.0101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x213be63dd80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = hypermodel.build(best_hyperparams[0])\n",
    "\n",
    "# fit model\n",
    "model.fit(data_train, y_train_categorical, epochs=50, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 649us/step\n",
      "Accuracy average: 0.3507362784471218\n",
      "Accuracy funny: 0.012048192771084338\n",
      "Accuracy somewhat funny: 0.7028112449799196\n",
      "Accuracy not funny: 0.3373493975903614\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "predictions = model.predict(data_test)\n",
    "\n",
    "# transform predictions to labels\n",
    "labels = []\n",
    "for prediction in predictions:\n",
    "    labels.append(prediction.argmax())\n",
    "\n",
    "# calculate accuracy\n",
    "cal_acc(df_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df_test columns 'pred' and 'label' to csv\n",
    "# df_test[['pred', 'label']].to_csv('result/nn_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
