{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from keras_tuner import HyperModel, Hyperband\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recode label successfully!\n"
     ]
    }
   ],
   "source": [
    "# read captions in the data\n",
    "df = pd.read_csv('data/all_data_850-870_embedding.csv')\n",
    "if 'funny' in df['label'].values:\n",
    "    df['label'] = df['label'].replace('funny', 2)\n",
    "    df['label'] = df['label'].replace('somewhat_funny', 1)\n",
    "    df['label'] = df['label'].replace('not_funny', 0)\n",
    "    print('Recode label successfully!')\n",
    "df['caption'] = df['caption'].astype(str)\n",
    "captions = df['caption'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  8431\n",
      "max_length:  73\n"
     ]
    }
   ],
   "source": [
    "# caculate the vocabulary size\n",
    "word_index = {}\n",
    "for caption in captions:\n",
    "    for word in caption.split():\n",
    "        if word not in word_index:\n",
    "            word_index[word] = 1\n",
    "        else:\n",
    "            word_index[word] += 1\n",
    "print(\"vocabulary size: \", len(word_index))\n",
    "\n",
    "# caculate the max length of captions\n",
    "max_length = 0\n",
    "for caption in captions:\n",
    "    if len(caption.split()) > max_length:\n",
    "        max_length = len(caption.split())\n",
    "print(\"max_length: \",max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "vocab_size = 6000   # Choose based on the vocabulary size of your dataset\n",
    "max_length = 15   # Choose based on the length of your longest caption\n",
    "embedding_dim = 300   # Size of the word embeddings\n",
    "num_classes = 3   # Number of funniness categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the captions\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['caption'])\n",
    "sequences = tokenizer.texts_to_sequences(df['caption'])\n",
    "text_data = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encode the labels\n",
    "labels= df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_training, X_test, y_training, y_test = train_test_split(\n",
    "    text_data, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge x_training and y_training\n",
    "train_data = np.column_stack((X_training, y_training))\n",
    "train_data = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = 6000\n",
    "        self.max_length = 15\n",
    "        self.embedding_dim = 300\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_length))\n",
    "        model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "        model.add(Dense(units=hp.Int('unit', min_value=64, max_value=256, step=32),\n",
    "                        activation=hp.Choice('activation', values=['relu', 'tanh']),\n",
    "                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=hp.Choice('l1', values=[0.01, 0.001, 0.0001, 0.0]),\n",
    "                                                                        l2=hp.Choice('l2', values=[0.01, 0.001, 0.0001, 0.0]))))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 24s]\n",
      "val_accuracy: 0.383584588766098\n",
      "\n",
      "Best val_accuracy So Far: 0.40033501386642456\n",
      "Total elapsed time: 00h 04m 02s\n"
     ]
    }
   ],
   "source": [
    "# Prepare your data\n",
    "X = train_data.iloc[:, :-1]\n",
    "y = train_data.iloc[:, -1]\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "best_hyperparams_per_fold = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(X)):\n",
    "    print(f\"Running tuning for fold {fold + 1}\")\n",
    "\n",
    "    # Split data into train and validation for the current fold\n",
    "    X_train, X_val = X.iloc[train_indices,], X.iloc[val_indices,]\n",
    "    y_train, y_val = y.iloc[train_indices,], y.iloc[val_indices,]\n",
    "\n",
    "    # Define the hypermodel\n",
    "    hypermodel = MyHyperModel(num_classes=3)\n",
    "\n",
    "    # Initialize the Hyperband tuner\n",
    "    tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=20,\n",
    "        directory=f'result/my_dir_{fold}',\n",
    "        project_name='hyperparameter_tuning',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    # Start the tuning process\n",
    "    tuner.search(X_train, y_train, validation_data=(X_val, y_val), class_weight={0: 1, 1: 1, 2: 1})\n",
    "\n",
    "    # Store the top 3 best hyperparameters of this fold\n",
    "    top_3_hyperparams_per_fold = tuner.get_best_hyperparameters(num_trials=3)\n",
    "    best_hyperparams_per_fold.append(top_3_hyperparams_per_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'unit': 256, 'activation': 'relu', 'l1': 0.0, 'l2': 0.0001, 'dropout': 0.0, 'learning_rate': 0.0001, 'tuner/epochs': 20, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
      "{'unit': 192, 'activation': 'relu', 'l1': 0.01, 'l2': 0.0, 'dropout': 0.0, 'learning_rate': 0.0001, 'tuner/epochs': 20, 'tuner/initial_epoch': 7, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0015'}\n",
      "{'unit': 96, 'activation': 'tanh', 'l1': 0.01, 'l2': 0.0, 'dropout': 0.4, 'learning_rate': 0.0001, 'tuner/epochs': 3, 'tuner/initial_epoch': 0, 'tuner/bracket': 2, 'tuner/round': 0}\n",
      "{'unit': 160, 'activation': 'relu', 'l1': 0.001, 'l2': 0.001, 'dropout': 0.30000000000000004, 'learning_rate': 0.0001, 'tuner/epochs': 20, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
      "{'unit': 256, 'activation': 'tanh', 'l1': 0.0001, 'l2': 0.001, 'dropout': 0.0, 'learning_rate': 0.01, 'tuner/epochs': 20, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n"
     ]
    }
   ],
   "source": [
    "# select best hyperparameters from best_hyperparams_per_fold\n",
    "best_hyperparams = []\n",
    "for top_hyperparams in best_hyperparams_per_fold:\n",
    "    best_hyperparams.append(top_hyperparams[0])\n",
    "\n",
    "# show best hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "for top_hyperparam in best_hyperparams:\n",
    "    print(top_hyperparam.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 1.7027 - accuracy: 0.3375 - val_loss: 1.2489 - val_accuracy: 0.3484\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 1.0351 - accuracy: 0.5921 - val_loss: 1.2256 - val_accuracy: 0.3618\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 0.7097 - accuracy: 0.7822 - val_loss: 1.3188 - val_accuracy: 0.3166\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 0.4918 - accuracy: 0.8492 - val_loss: 1.4454 - val_accuracy: 0.3518\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 0.3638 - accuracy: 0.8945 - val_loss: 1.7730 - val_accuracy: 0.3618\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 0.3382 - accuracy: 0.9049 - val_loss: 2.6468 - val_accuracy: 0.3685\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.3114 - accuracy: 0.9188 - val_loss: 2.5696 - val_accuracy: 0.3601\n",
      "Epoch 8/20\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.2538 - accuracy: 0.9280 - val_loss: 3.0890 - val_accuracy: 0.3434\n",
      "Epoch 9/20\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.2187 - accuracy: 0.9426 - val_loss: 3.3084 - val_accuracy: 0.3501\n",
      "Epoch 10/20\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.1971 - accuracy: 0.9506 - val_loss: 3.8914 - val_accuracy: 0.3534\n",
      "Epoch 11/20\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.1866 - accuracy: 0.9510 - val_loss: 4.2894 - val_accuracy: 0.3551\n",
      "Epoch 12/20\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.1742 - accuracy: 0.9569 - val_loss: 3.8840 - val_accuracy: 0.3668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23b31d54bb0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = hypermodel.build(best_hyperparams[4])\n",
    "\n",
    "# fit model\n",
    "model.fit(X_training, y_training, epochs=20, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# show the predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# convert the predictions to labels\n",
    "predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the predictions with the y_test\n",
    "df_result = pd.DataFrame(columns=['label', 'prediction'])\n",
    "df_result['label'] = y_test\n",
    "df_result['prediction'] = predictions\n",
    "\n",
    "# add a column for accuracy\n",
    "df_result['correct'] = np.where(df_result['label'] == df_result['prediction'], 1, 0)\n",
    "accuracy = df_result['correct'].sum() / len(df_result)\n",
    "\n",
    "# calculate the accuracy for each class\n",
    "df_result['correct'] = df_result['label'] == df_result['prediction']\n",
    "df_result['correct'] = df_result['correct'].astype(int)\n",
    "accuracy_class = df_result.groupby('label')['correct'].sum() / df_result.groupby('label')['correct'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy average:  0.3119143239625167\n",
      "Accuracy funny:  0.03076923076923077\n",
      "Accuracy somewhat funny:  0.9106382978723404\n",
      "Accuracy not funny:  0.04365079365079365\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy average: ', accuracy)\n",
    "print('Accuracy funny: ', accuracy_class[2])\n",
    "print('Accuracy somewhat funny: ', accuracy_class[1])\n",
    "print('Accuracy not funny: ', accuracy_class[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
