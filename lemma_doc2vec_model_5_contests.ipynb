{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f62078",
   "metadata": {},
   "source": [
    "INTRODUCTION\n",
    "\n",
    "The goal of this analysis is to show an alternative method to create caption embeddings or vectors compared to using a pretrained Sentence-Bert model. I will use the term embeddings and vectors interchangeably. By capturing embeddings for captions in multiple contests, we can use these to infer on what makes a caption funny. To create vectors, I will use an algorithm called Doc2Vec which generates vectors for each individual caption. The data is grabbed from our SQL database in which the data was collected from https://nextml.github.io/caption-contest-data/. In this notebook, I will use data from 5 contests since this version is used for demonstration purposes. The difference from this notebook to the other Doc2Vec model is the step of lemmatizing our data before inserting them into the model. This is to see the difference in vectors based on the regular model and the lemmatized model.\n",
    "\n",
    "The first step of this analysis is to load in the relevant libraries and pull down data from the SQL database in which the next two blocks do. I am using the gensim package for preprocessing and topic modeling which is an open source Python library representing documents as semantic vectors, as efficiently and painlessly as possible. It is designed to process raw, unstructured digital texts (“plain text”) using unsupervised machine learning algorithms. This is the fastest library for natural language processing and it is easy to use and understand. Next, I am requesting a connection to the SQL database by using a Python package called mysql.connector which allows Python progams to have access to SQL databases. The database I am pulling down information from is called new york cartoon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0371ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for the doc2vec model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict \n",
    "from numpy import dot\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde42262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to SQL database\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "try:\n",
    "    connection = mysql.connector.connect(host='dbnewyorkcartoon.cgyqzvdc98df.us-east-2.rds.amazonaws.com',\n",
    "                                         database='new_york_cartoon',\n",
    "                                         user='dbuser',\n",
    "                                         password='Sql123456')\n",
    "    if connection.is_connected():\n",
    "        db_Info = connection.get_server_info()\n",
    "        print(\"Connected to MySQL Server version \", db_Info)\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"select database();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(\"You succeed to connect to database: \", record)\n",
    "\n",
    "except Error as e:\n",
    "    print(\"Error while connecting to MySQL\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005b12c",
   "metadata": {},
   "source": [
    "In order to understand how we can get our data from SQL, we have to input what contest numbers we want our captions from. In this case, we want data from the last 5 contests. We can do this using SQL's search function and selecting the result table which allows to get data from the contests and show it in a Pandas dataframe for ease of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d979c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling down data from SQL database via search\n",
    "sql_select_Query = \"select caption,ranking from result where contest_num in (863, 862, 861, 860, 859);\"  # you can change query in this line for selecting your target data\n",
    "cursor.execute(sql_select_Query)\n",
    "\n",
    "# show attributes names of target data\n",
    "num_attr = len(cursor.description)\n",
    "attr_names = [i[0] for i in cursor.description]\n",
    "print(attr_names)\n",
    "\n",
    "# get all records\n",
    "records = cursor.fetchall()\n",
    "print(\"Total number of rows in table: \", cursor.rowcount)\n",
    "df = pd.DataFrame(records, columns=attr_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bcdea",
   "metadata": {},
   "source": [
    "The second step of this analysis is to first get rid of any distracting information or columns that are not useful to us in text extraction. As we can see in our dataframe (df), we have two columns called caption and ranking in which the former contains the text that we want to analyze. The second column, ranking, is not needed in our analysis because its values contain numbers which do not contain any meaningful information. Therefore, I will drop the \"ranking\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ff6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unneccessary columns, axis = 1 means to remove vertical axis(columns)\n",
    "df = df.drop(columns=['ranking'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f8f1f",
   "metadata": {},
   "source": [
    "Next, we have to perform some preprocessing of our text. Preprocessing of text before any form of analysis is very important because it can remove noise such as unnecessary punctuation which contain no meaning. It also allows us to homogenize all the words through lowercasing them. Having uppercase letters might cause variation in how the text is analyzed which can cause different results in our embeddings. Apparently, the values in the column are classified as objects when they should be strings, so I will convert them to strings before performing preprocessing. Here, I am creating a new column called \"caption_processed\" because I want to see how the text changes once we have finished our preprocessing for clarity purposes. I am using the re library to substitute all the punctuation in the brackets with a blank space and I am lowercasing all words using the lower function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation lowercasing and creating new column \"caption_processed\"\n",
    "df['caption'] = df['caption'].astype(str)\n",
    "df['caption_processed'] = df['caption'].map(lambda x: re.sub(r'[,\\.\\!\\?\\'\\\"]', '', x).lower())\n",
    "df['caption_processed'] = df['caption_processed'].map(lambda x: re.sub(r'[--]', ' ', x).lower())\n",
    "\n",
    "# Print out the first rows of captions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642e309",
   "metadata": {},
   "source": [
    "In the next few code blocks, I am simply preprocessing the text even more. First, I am making the values in the caption_processed column into a list for tokenization. I am using using the simple_preprocess function from gensim which tokenizes text and passing it through an interative for loop. Then, I'm making a list of tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing and cleaning up text\n",
    "data = df.caption_processed.values.tolist()\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e107e",
   "metadata": {},
   "source": [
    "Next, I created two objects that captures bigrams and trigrams. Bigrams are phrases that have two words appear in pairs consecutively and trigrams are phrases that have three words appear together consecutively. There might be some bigrams and trigrams in our data, and I want to cover all of our data so I don't miss any patterns. I set the min_count to 5 and threshold to 100 because having a lower appearance rate ensures that not all phrases become bigrams/trigrams by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42757ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold = fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fffda05",
   "metadata": {},
   "source": [
    "Stopwords are a set of commonly used words in any language in this case, English. Removing stopwords is very important in text processing as it can remove noise from the data and provide greater semantical meaning with those words removed. The most common corpus used for stopwords is NLTK's dictionary, but I have opted to use Spacy's dictionary instead. Spacy's dictionary of stopwords is larger thus potentially removing more noise from the data and having a cleaner look at the most important words. I load stopwords from the Spacy library and choose stopwords in English since our text is in English. I am using Spacy's model \"en_core_web_sm\" which is a small English pipeline trained on written web text that includes vocabulary, syntax and entities. I am using the small model for faster computational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading stopwords from Spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d113ff",
   "metadata": {},
   "source": [
    "I created functions removing stopwords, creating bigram phrases, and lemmatizing words. I then applied them to my list of processed words. The next step is to lemmatize the text. Lemmatization is a preprocessing method that reduces words to their root form. By lemmatizing words before tokenizing allows for efficient and faster processing afterwards. I am creating a function called \"lemmatization\" that takes in nouns, adjectives, verbs, adjectives, and proper nouns from Spacy's vocabulary. In this function, I am lemmatizing words that are in the text and in \"allowed_postags\" and then appending them into an empty list \"texts_out\" which will contain all the new lemmatized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318daba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6112d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv, propn\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN'])\n",
    "print(data_lemmatized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a4c4c",
   "metadata": {},
   "source": [
    "The final step before creating the Doc2Vec model is to make a unique id for each individual caption. This is so that the model can know that each caption is a unique item and it generates an embedding for each one. I did this through the tagged_document function which creates a tag or \"id\" for each caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0914e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating document ids\n",
    "def tagged_document(documents):\n",
    "    for i, words in enumerate(documents):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(words, [i])\n",
    "\n",
    "tagged_documents = list(tagged_document(documents))\n",
    "\n",
    "# Print the first TaggedDocument\n",
    "print(tagged_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4997992",
   "metadata": {},
   "source": [
    "This block is for viewing the vectors in their raw state which consumes a lot of memory. If you wish to view the vectors and see how they look like, please uncomment this code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd814f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, tagged_doc in enumerate(d_vectors):\n",
    "    # words = tagged_doc.words\n",
    "    # print(f\"Words in Document {i}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a0834",
   "metadata": {},
   "source": [
    "We can now create our Doc2Vec model. Some parameters to take note of are the window parameter which is how many words surrond the target word and vector size which is the length of our embeddings. Other parameters to understand is the dm parameter which makes the model use Distrbuted Memory (DM) to create embeddings based on the context of the caption which gives us the best embedding to use. I ran an epochs of 15 so that it trains itself a sufficient amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size = 200, \n",
    "                                      window = 10,\n",
    "                                      min_count = 5, \n",
    "                                      dm = 1,\n",
    "                                      dbow_words = 0,\n",
    "                                      epochs = 15,\n",
    "                                      workers = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7f96c",
   "metadata": {},
   "source": [
    "Afterwards, we load our tagged documents into the model and build its vocabulary for training. We then train the model on its vocabulary and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model vocabulary\n",
    "model.build_vocab(tagged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4bf93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644047c",
   "metadata": {},
   "source": [
    "Finally, once we have our vectors we can save them into a numpy file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05417dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving caption vectors to a compressed numpy file\n",
    "def get_document_vectors_with_ids(model, tagged_docs):\n",
    "    document_vectors = []\n",
    "    for i, (doc_id, dv) in enumerate(zip(tagged_docs, model.dv)):\n",
    "        words = doc_id.words\n",
    "        document_vectors.append((f\"Document {i + 1}\", words, dv))\n",
    "    return document_vectors\n",
    "\n",
    "document_vectors_with_ids = get_document_vectors_with_ids(model, tagged_documents)\n",
    "\n",
    "dtype = [('doc_id', 'U20'), ('words', object), ('doc_vector', np.float32, (model.vector_size,))]\n",
    "data = np.array(document_vectors_with_ids, dtype=dtype)\n",
    "\n",
    "# Save the data as an NPZ file\n",
    "np.savez(\"caption_vectors_lemma.npz\", data=data)\n",
    "\n",
    "print(\"Document IDs, vectors, and words saved to 'caption_vectors_lemma.npz'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
