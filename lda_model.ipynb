{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2645516",
   "metadata": {},
   "source": [
    "INTRODUCTION\n",
    "\n",
    "The goal of this analysis is to demonstrate the usage of topic modeling using Python on the New Yorker Caption Contest database. By performing topic modeling on multiple contests, we can see which topics are common across them and perhaps in the future use this information to infer on what makes a caption funny. To perform topic modeling, I will use an algorithm called Latent Dirichlet Allocation (LDA) to grab topic vectors and visualize common topics. The data is collected from https://nextml.github.io/caption-contest-data/ and is stored on a SQL database. \n",
    "\n",
    "I am using the gensim package for preprocessing and topic modeling which is an open source Python library representing documents as semantic vectors, as efficiently and painlessly as possible. It is designed to process raw, unstructured digital texts (“plain text”) using unsupervised machine learning algorithms. This is the fastest library for natural language processing and it is easy to use and understand.\n",
    "\n",
    "The first step of this analysis is to pull down our data from the SQL database in which the code block below does so. I am requesting a connection to the SQL database by using a Python package called mysql.connector which allows Python progams to have access to SQL databases. The database I am pulling down information from is called new york cartoon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for topic modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict \n",
    "from numpy import dot\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "from pyLDAvis import save_html\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to SQL database\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "try:\n",
    "    connection = mysql.connector.connect(host='dbnewyorkcartoon.cgyqzvdc98df.us-east-2.rds.amazonaws.com',\n",
    "                                         database='new_york_cartoon',\n",
    "                                         user='dbuser',\n",
    "                                         password='Sql123456')\n",
    "    if connection.is_connected():\n",
    "        db_Info = connection.get_server_info()\n",
    "        print(\"Connected to MySQL Server version \", db_Info)\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"select database();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(\"You succeed to connect to database: \", record)\n",
    "\n",
    "except Error as e:\n",
    "    print(\"Error while connecting to MySQL\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422db6f0",
   "metadata": {},
   "source": [
    "In order to understand how we can get our data from SQL, we have to input what contest numbers we want our captions from. In this case, we want data from all contests. We can do this using SQL's search function and selecting the result table which allows to get data from the contests and show it in a Pandas dataframe for ease of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa102dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling down data from SQL database via search\n",
    "sql_select_Query = \"select caption,ranking from result;\"\n",
    "cursor.execute(sql_select_Query)\n",
    "\n",
    "# show attributes names of target data\n",
    "num_attr = len(cursor.description)\n",
    "attr_names = [i[0] for i in cursor.description]\n",
    "print(attr_names)\n",
    "\n",
    "# get all records\n",
    "records = cursor.fetchall()\n",
    "print(\"Total number of rows in table: \", cursor.rowcount)\n",
    "df = pd.DataFrame(records, columns=attr_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1206749",
   "metadata": {},
   "source": [
    "Next, we have to perform some preprocessing of our text. Preprocessing of text before any form of analysis is very important because it can remove noise such as unnecessary punctuation which contain no meaning. It also allows us to homogenize all the words through lowercasing them. Having uppercase letters might cause variation in how the text is analyzed which can cause different results in our embeddings. Apparently, the values in the column are classified as objects when they should be strings, so I will convert them to strings before performing preprocessing. Here, I am creating a new column called \"caption_processed\" because I want to see how the text changes once we have finished our preprocessing for clarity purposes. I am using the re library to substitute all the punctuation in the brackets with a blank space and I am lowercasing all words using the lower function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation lowercasing and creating new column \"caption_processed\"\n",
    "df['caption'] = df['caption'].astype(str)\n",
    "df['caption_processed'] = df['caption'].map(lambda x: re.sub(r'[,\\.\\!\\?\\\"\\']', '', x).lower())\n",
    "df['caption_processed'] = df['caption_processed'].map(lambda x: re.sub(r'[--]', ' ', x).lower())\n",
    "\n",
    "# Print out the first rows of captions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cbec8e",
   "metadata": {},
   "source": [
    "In the next few code blocks, I am simply preprocessing the text even more. First, I am making the values in the caption_processed column into a list for tokenization. I am using using the simple_preprocess function from gensim which tokenizes text and passing it through an interative for loop. Then, I'm making a list of tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing and cleaning up text\n",
    "data = df.caption_processed.values.tolist()\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85699c6",
   "metadata": {},
   "source": [
    "Next, I created two objects that captures bigrams and trigrams. Bigrams are phrases that have two words appear in pairs consecutively and trigrams are phrases that have three words appear together consecutively. There might be some bigrams and trigrams in our data, and I want to cover all of our data so I don't miss any patterns. I set the min_count to 5 and threshold to 100 because having a lower appearance rate ensures that not all phrases become bigrams/trigrams by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold = fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a27e0d",
   "metadata": {},
   "source": [
    "Stopwords are a set of commonly used words in any language in this case, English. Removing stopwords is very important in text processing as it can remove noise from the data and provide greater semantical meaning with those words removed. The most common corpus used for stopwords is NLTK's dictionary, but I have opted to use Spacy's dictionary instead. Spacy's dictionary of stopwords is larger thus potentially removing more noise from the data and having a cleaner look at the most important words. I load stopwords from the Spacy library and choose stopwords in English since our text is in English. I am using Spacy's model \"en_core_web_sm\" which is a small English pipeline trained on written web text that includes vocabulary, syntax and entities. I am using the small model for faster computational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading stopwords from Spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stop_words = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b9a16",
   "metadata": {},
   "source": [
    "I created functions removing stopwords, creating bigram phrases, and lemmatizing words. I then applied them to my list of processed words. The next step is to lemmatize the text. Lemmatization is a preprocessing method that reduces words to their root form. By lemmatizing words before tokenizing allows for efficient and faster processing afterwards. I am creating a function called \"lemmatization\" that takes in nouns, adjectives, verbs, adjectives, and proper nouns from Spacy's vocabulary. In this function, I am lemmatizing words that are in the text and in \"allowed_postags\" and then appending them into an empty list \"texts_out\" which will contain all the new lemmatized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6274f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv, propn\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN'])\n",
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6916751",
   "metadata": {},
   "source": [
    "Before building our LDA model, we have to create a dictionary called id2word which allows to look up individual words and their frequency in the text. This is a useful tool in seeing what words are most frequent throughout the text and if some patterns of words appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "texts = data_lemmatized\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceff5b9",
   "metadata": {},
   "source": [
    "Now that the text has been preprocessed and a corpus has been made, I can build an LDA model. The model has some necessary parameters which include the id2word dictionary and corpus. You may notice that num_topics which represent the number of topics in the model is 20 but this number is arbitrary meaning that you can put in whatever number and the model will return the number of topics specified. This is a problem that will be fixed in the next steps. Other parameters include random_state which is a seed for replicability, chunksize which is the number of captions in a training batch, passes which is how many times the model is passed over for training. I am using the multicore version of Gensim's LDA model because of parallel processing which speeds of computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lda model\n",
    "lda_model = models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                    id2word=id2word,\n",
    "                                    num_topics=20,\n",
    "                                    random_state=100,\n",
    "                                    chunksize=100,\n",
    "                                    passes=10,\n",
    "                                    workers=6, \n",
    "                                    per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171b3e7",
   "metadata": {},
   "source": [
    "How do you know if a topics model is good? I can find out if my topics model is good through its perplexity and coherence score. Perplexity is a measure of how good a model is and coherence score measures how clear the topcis are. I want a low perplexity score and a high coherence score which indicates a good model. To adjust these scores, I need to tune the chunksize and passes parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cecb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v') # a measure of how accurate the model is. higher is better\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ab5f9",
   "metadata": {},
   "source": [
    "I have a problem of creating a model because I do not know the optimal amount of topics that are meaningful and interpretable. To fix this problem, I created a function that creates multiple models with each having different amount of topics starting from 2 and ending at 50 topics. For each model, I also measured their respective coherence scores to see which model is the best that I can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(50)\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                    id2word=id2word,\n",
    "                                    num_topics=num_topics,\n",
    "                                    random_state=100,\n",
    "                                    chunksize=100,\n",
    "                                    passes=10,\n",
    "                                    workers=6, \n",
    "                                    per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    # Find the model with the highest coherence value\n",
    "    optimal_model_index = coherence_values.index(max(coherence_values))\n",
    "    optimal_model = model_list[optimal_model_index]\n",
    "\n",
    "    return model_list, coherence_values, optimal_model\n",
    "\n",
    "model_list, coherence_values, optimal_model = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=50, step=6)\n",
    "\n",
    "for num_topics, cv in zip(range(2, 50, 6), coherence_values):\n",
    "    print(\"Num Topics =\", num_topics, \" has Coherence Value of\", round(cv, 3))\n",
    "\n",
    "print(\"Optimal Model:\", optimal_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9cefd",
   "metadata": {},
   "source": [
    "I can also visualize the optimal LDA model by graphing its coherence score along with the number of topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb7e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit=50; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6689d7b",
   "metadata": {},
   "source": [
    "Now that I found the best number of topics for my model, I can visualize the model using the pyLDAvis library which is exclusive to LDA models. The \"R=20\" parameter indicates how many terms I want to show each topic bubble. In this case, I want to show the top 20 words for each topic. I exported the model's visualization as an html file for sharing with other people in my team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the visualization\n",
    "lda_display = gensimvis.prepare(optimal_model, corpus, id2word, mds=\"mmds\", R=20, sort_topics=False)\n",
    "    \n",
    "# Generate a filename based on the current topic number\n",
    "filename = f\"lda_vis.html\"\n",
    "\n",
    "# Save the HTML visualization with the topic number in the filename\n",
    "pyLDAvis.save_html(lda_display, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef613bd0",
   "metadata": {},
   "source": [
    "With this model, I can find a lot of interesting information of the topics in the captions. One of the practical application of topic modeling is to determine what topic a given document is about. To find that, I find the topic number that has the highest percentage contribution in that caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016df12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the dominant topic in each individual caption\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    data = []\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "        topics = ldamodel.get_document_topics(doc)\n",
    "        topics = sorted(topics, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Initialize variables to store dominant topic and its contribution\n",
    "        dominant_topic = -1\n",
    "        max_topic_contribution = 0.0\n",
    "\n",
    "        for j, (topic_num, prop_topic) in enumerate(topics):\n",
    "            if j == 0:  # First topic is the dominant topic\n",
    "                dominant_topic = topic_num\n",
    "                max_topic_contribution = prop_topic\n",
    "\n",
    "        # Get the keywords for the dominant topic\n",
    "        wp = ldamodel.show_topic(dominant_topic)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "\n",
    "        # Append the data as a list\n",
    "        data.append([int(dominant_topic), round(max_topic_contribution, 4), topic_keywords, texts[i]])\n",
    "\n",
    "    sent_topics_df = pd.DataFrame(data, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return sent_topics_df\n",
    "\n",
    "# Example usage:\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "df_dominant_topic.to_csv('Dominant_Topic_in_each_Caption.csv', index=False)\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba88d2f",
   "metadata": {},
   "source": [
    "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, I can find the documents a given topic has contributed to the most and infer the topic by reading that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most representive caption of each topic\n",
    "sent_topics_sorted = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorted = pd.concat([sent_topics_sorted, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorted.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "sent_topics_sorted.to_csv('Most_Representative_Caption_of_Each_Topic.csv', index=False)\n",
    "\n",
    "# Show\n",
    "sent_topics_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efcdf0",
   "metadata": {},
   "source": [
    "I also want to understand the volume and distribution of topics in order to judge how widely it was discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = sent_topics_sorted[['Topic_Num', 'Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "df_dominant_topics.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Topic_Num', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "df_dominant_topics.to_csv('Topic_Distributions.csv', index=False)\n",
    "\n",
    "# Show\n",
    "df_dominant_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c9081",
   "metadata": {},
   "source": [
    "I am most interested in finding the probability of each topic showing up in each caption. I can do so by using the show_topics function which returns the topic probability for each caption. I added in the \"ranking column\" from our original dataframe for more information. Additionally, I'm also interested in finding the top 20 words for each topic. Lastly, I compiled all of this data in to a csv for others to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding topic probabilities in each caption\n",
    "def corpus_to_lda_features(lda_model, corpus, num_words=20):\n",
    "    topic_probabilities_list = []\n",
    "\n",
    "    # Get the top words for each topic with probabilities\n",
    "    topic_terms = lda_model.show_topics(num_topics=lda_model.num_topics, num_words=num_words, formatted=False)\n",
    "\n",
    "    # Extracting top words and probabilities for each topic\n",
    "    top_words_per_topic = {topic[0]: [(word[0], word[1]) for word in topic[1]] for topic in topic_terms}\n",
    "\n",
    "    for doc in corpus:\n",
    "        topic_probabilities = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "        topic_probabilities = np.array(topic_probabilities)\n",
    "        topic_probabilities_list.append(topic_probabilities[:, 1])\n",
    "\n",
    "    # Create a pandas DataFrame with a column for each topic\n",
    "    df = pd.DataFrame(topic_probabilities_list)\n",
    "\n",
    "    # Add columns for top words and their probabilities of each topic\n",
    "    for topic, top_words_probs in top_words_per_topic.items():\n",
    "        top_words, word_probs = zip(*top_words_probs)\n",
    "        df[f\"Topic_{topic}_top_words\"] = pd.Series(top_words)\n",
    "        df[f\"Topic_{topic}_word_probs\"] = pd.Series(word_probs)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_probabilities_df = corpus_to_lda_features(optimal_model, corpus, num_words = 20)\n",
    "topic_probabilities_df = topic_probabilities_df.assign(caption_text=df.caption.values)\n",
    "topic_probabilities_df = topic_probabilities_df.assign(ranking=df.ranking)\n",
    "\n",
    "column_name = 'caption_text'\n",
    "first_column = topic_probabilities_df.pop(column_name)\n",
    "second_column = topic_probabilities_df.pop('ranking')\n",
    "topic_probabilities_df.insert(0, column_name, first_column)\n",
    "topic_probabilities_df.insert(1, 'ranking', second_column)\n",
    "topic_probabilities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_probabilities_df.to_csv('document_topics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
